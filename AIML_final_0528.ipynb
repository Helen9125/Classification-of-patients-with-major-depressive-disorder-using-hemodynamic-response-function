# -*- coding: utf-8 -*-
"""
Created on Tue May 28 04:22:43 2024

@author: stu92
"""

import pandas as pd
import numpy as np
from sklearn import svm
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import RFE, SelectKBest, chi2, SelectFromModel
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# File path to the Excel file
file_path = "D:\\AIML\\Final_project\\Dataset\\All_parameters_cleaned.xlsx"

# Load the data from the Excel file
df = pd.read_excel(file_path)

# Assuming the Excel file has sheets named 'Features' and 'Labels'
# Adjust this based on your actual file structure
X = df.iloc[:, 4:274].values
y = df.iloc[:, 1].values

# Reshape X if necessary
num_samples = X.shape[0]
num_features = X.shape[1] * X.shape[2] if len(X.shape) > 2 else X.shape[1]
X = X.reshape(num_samples, num_features)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# #Uncomment to use PCA for feature selection
# #Define PCA with the number of components you want to keep
# pca = PCA(n_components=0.95)  # Retain 95% of variance
# #Apply PCA to the training data
# X_train = pca.fit_transform(X_train)
# X_test = pca.transform(X_test)

# #Uncomment to use LASSO for feature selection
# #Define LASSO with a regularization parameter alpha
# lasso = Lasso(alpha=0.01)
# #Fit LASSO on the training data
# lasso.fit(X_train, y_train)
# #Get the coefficients from the LASSO model
# lasso_coef = lasso.coef_
# #Select features where coefficients are not zero
# selected_features = np.where(lasso_coef != 0)[0]
# #Reduce training and testing data to selected features
# X_train = X_train[:, selected_features]
# X_test = X_test[:, selected_features]

#Uncomment to use RFE for feature selection
#Initialize the base model for RFE
base_model = svm.SVC(kernel='linear')
#Initialize RFE with the base model
rfe = RFE(estimator=base_model, n_features_to_select=10)
#Fit RFE on the training data
rfe.fit(X_train, y_train)
#Reduce training and testing data to selected features
X_train = rfe.transform(X_train)
X_test = rfe.transform(X_test)

# #Uncomment to use SelectKBest for feature selection
# #Initialize SelectKBest with chi-square scoring function
# skb = SelectKBest(score_func=chi2, k=10)
# #Fit SelectKBest on the training data
# skb.fit(X_train, y_train)
# #Reduce training and testing data to selected features
# X_train = skb.transform(X_train)
# X_test = skb.transform(X_test)

# #Uncomment to use SelectFromModel for feature selection
# #Initialize ExtraTreesClassifier
# etc = ExtraTreesClassifier(n_estimators=50)
# #Fit the model on the training data
# etc.fit(X_train, y_train)
# #Initialize SelectFromModel with the trained ExtraTreesClassifier
# sfm = SelectFromModel(etc, threshold="mean")
# #Fit SelectFromModel on the training data
# sfm.fit(X_train, y_train)
# #Reduce training and testing data to selected features
# X_train = sfm.transform(X_train)
# X_test = sfm.transform(X_test)

# SVM model with hyperparameter tuning
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma': ['scale', 'auto']
}
grid_search_svm = GridSearchCV(svm.SVC(), param_grid_svm, cv=5, scoring='accuracy')
grid_search_svm.fit(X_train, y_train)
best_model_svm = grid_search_svm.best_estimator_
y_pred_svm = best_model_svm.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'Best SVM Parameters: {grid_search_svm.best_params_}')
print(f'SVM Accuracy: {accuracy_svm * 100:.2f}%')
cm_svm = confusion_matrix(y_test, y_pred_svm)
print('SVM Confusion Matrix:')
print(cm_svm)
report_svm = classification_report(y_test, y_pred_svm)
print('SVM Classification Report:')
print(report_svm)

# # Random Forest model with hyperparameter tuning
# param_grid_rf = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4]
# }
# grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy')
# grid_search_rf.fit(X_train, y_train)
# best_model_rf = grid_search_rf.best_estimator_
# y_pred_rf = best_model_rf.predict(X_test)
# accuracy_rf = accuracy_score(y_test, y_pred_rf)
# print(f'Best Random Forest Parameters: {grid_search_rf.best_params_}')
# print(f'Random Forest Accuracy: {accuracy_rf * 100:.2f}%')
# cm_rf = confusion_matrix(y_test, y_pred_rf)
# print('Random Forest Confusion Matrix:')
# print(cm_rf)
# report_rf = classification_report(y_test, y_pred_rf)
# print('Random Forest Classification Report:')
# print(report_rf)

# KNN model with hyperparameter tuning
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}
grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train, y_train)
best_model_knn = grid_search_knn.best_estimator_
y_pred_knn = best_model_knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'Best KNN Parameters: {grid_search_knn.best_params_}')
print(f'KNN Accuracy: {accuracy_knn * 100:.2f}%')
cm_knn = confusion_matrix(y_test, y_pred_knn)
print('KNN Confusion Matrix:')
print(cm_knn)
# report_knn = classification_report(y_test, y_pred_knn)
# print('KNN Classification Report:')
# print(report_knn)

# Logistic Regression model with hyperparameter tuning
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'lbfgs', 'liblinear'],
    'penalty': ['l2']
}
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')
grid_search_lr.fit(X_train, y_train)
best_model_lr = grid_search_lr.best_estimator_
y_pred_lr = best_model_lr.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f'Best Logistic Regression Parameters: {grid_search_lr.best_params_}')
print(f'Logistic Regression Accuracy: {accuracy_lr * 100:.2f}%')
cm_lr = confusion_matrix(y_test, y_pred_lr)
print('Logistic Regression Confusion Matrix:')
print(cm_lr)
# report_lr = classification_report(y_test, y_pred_lr)
# print('Logistic Regression Classification Report:')
# print(report_lr)
